{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 03: Tokenization and word counts\n",
    "\n",
    "## To do\n",
    "\n",
    "* Readings for Wednesday\n",
    "  * Ramsay (Canvas), Healy, Rambsy\n",
    "  * Come to lecture prepared: have a question, thought, or connection between the readings\n",
    "  * No formal responses for this week\n",
    "* HW1 due via CMS by Thursday night (11:59pm)\n",
    "* Remaining from lecture 02:\n",
    "  * Review Pandas\n",
    "\n",
    "## Definition\n",
    "\n",
    "What is a token?\n",
    "\n",
    "* The **smallest individually meaningful unit of a document.** Roughly, a word.\n",
    "* But ... as soon as you see \"meaningful,\" you know it's going to be a matter of interpretation.\n",
    "  * *Every single thing you do in text analysis is an interpretive intervention!*\n",
    "* Not all tokens are (single) words. For example:\n",
    "  * **Contractions**. `\"I'm\"` or `\"can't\"`. One token or two?\n",
    "  * **Phrases.** `\"San Francisco\"` or `\"Cornell University\"`. Two tokens or one?\n",
    "    * These are exampled of \"named entities.\" We'll revisit them later in the semester.\n",
    "  * **Punctuation.** Count it at all? Is `\"this\"` the same token as `\"this!\"`?\n",
    "  * **Domain-specific terms.** `\"@user\"`, `\"COVID-19\"`, etc.\n",
    "\n",
    "## Why tokenize?\n",
    "\n",
    "Words suggest meaning. This is the wager and the starting point of many text analysis methods.\n",
    "  * If we can identify words, we can count them.\n",
    "  * If we we can count words, we can quantify (aspects of) a text that contains those words.\n",
    "  * If we can quantify a text, we can compute with it.\n",
    "\n",
    "Note that quantifying a text isn't the same thing as being *correct* about what that text means, nor is meaning solely a function of word counts(!).\n",
    "\n",
    "Tokenization is part of the more-or-less standard text-processing workflow. Other parts of that workflow might include:\n",
    "  * Case regularization/folding\n",
    "  * Punctuation removal\n",
    "  * Lemmatization or stemming\n",
    "  * Sentence segmentation\n",
    "  * and more ...\n",
    "\n",
    "## Tokenization can be domain-specific\n",
    "\n",
    "Note that today's reading assumed some special interests:\n",
    "\n",
    "* Twitter(like) texts\n",
    "* Sentiment as target phenomenon\n",
    "\n",
    "So it worked hard to capture Twitter handles, hashtags, smilies, URLs, etc.\n",
    "\n",
    "The \"right\" way to tokenize depends on your project, on what is meaningful *in context*.\n",
    "If you have different data or different phenomena to investigate, you might tokenize differently.\n",
    "\n",
    "## Approach 1: Split on whitespace\n",
    "\n",
    "A simple, naïve approach, workable for quick-and-dirty work with Western languages.\n",
    "\n",
    "Consider the sentence:\n",
    "\n",
    "> Cornell is a private, Ivy League university and the land-grant university for New York state.\n",
    "\n",
    "How many tokens does this sentence contain? (count them for yourself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cornell', 'is', 'a', 'private,', 'Ivy', 'League', 'university', 'and', 'the', 'land-grant', 'university', 'for', 'New', 'York', 'state.']\n",
      "Number of tokens: 15\n"
     ]
    }
   ],
   "source": [
    "cornell = 'Cornell is a private, Ivy League university and the land-grant university for New York state.'\n",
    "tokens = cornell.split()\n",
    "print(tokens)\n",
    "print(\"Number of tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: `private,` `land-grant` `state.` These aren't wrong *per se*, but ...\n",
    "\n",
    "Maybe we could do better if we just took non-space, non-puctuation strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cornell', 'is', 'a', 'private', 'Ivy', 'League', 'university', 'and', 'the', 'land', 'grant', 'university', 'for', 'New', 'York', 'state']\n",
      "Number of tokens: 16\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "word_pattern = re.compile(\"[\\w]+\")\n",
    "tokens_re = word_pattern.findall(cornell)\n",
    "print(tokens_re)\n",
    "print(\"Number of tokens:\", len(tokens_re))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions\n",
    "\n",
    "A totally inadequate mini-introduction to an important but annoyingly complex technology.\n",
    "\n",
    "* What is a regular expression (regex)?\n",
    "  * A sequence of characters that define a search pattern.\n",
    "  * That is, it's a text search or matching language.\n",
    "  * Notoriously unreadable and difficult to parse by eye.\n",
    "  \n",
    "Consider the line above:\n",
    "\n",
    "```\n",
    "word_pattern = re.compile(\"[\\w]+\")\n",
    "```\n",
    "\n",
    "The search pattern here is any sequence of one or more (`+`) uniterrupted \"word\" characters (`\\w` = upper- and lowercase letters, plus digits) that occur anywhere in a string. Regexes are usually \"greedy,\" so will continue matching character by character until their condition is not met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t']\n",
      "['the']\n",
      "['these']\n",
      "['these', 'uns']\n",
      "['these', 'ones']\n"
     ]
    }
   ],
   "source": [
    "for word in ['t', 'the', 'these', \"these'uns\", \"these ones\"]:\n",
    "    print(word_pattern.findall(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`re` is Python's regular expression library. `compile` prepares the regular expression for use with text inputs.\n",
    "\n",
    "A few other useful bits of regex syntax:\n",
    "\n",
    "* `.` (period) = any character\n",
    "* `\\s` = whitespace character (space, tab, newline, etc.)\n",
    "* `\\d` = digit\n",
    "* `[abc]` = any character in the set {a, b, c}.\n",
    "* `[^abc]` = negation, any character *except* a, b, or c.\n",
    "* `A*` = zero or more occurrences of the character A; `+` = one or more, `?` = zero or one.\n",
    "* `\\A`, `\\Z`, `^`, and `$` = match only at start or end of a string or line, respectively.\n",
    "* `\\` (backslash) = escape the next character; `\\.` = period, not wildcard.\n",
    "\n",
    "There's a lot more to this. Take a look at the code linked from today's reading, and/or consult a [regex cheat sheet](https://learnbyexample.github.io/cheatsheet/python/python-regex-cheatsheet/).\n",
    "\n",
    "Why use regular expressions?\n",
    "  * A powerful way to find/match/extract substrings from strings and texts.\n",
    "  * Can use regexes to build robust custom tokenizers (as in the reading for today)\n",
    "\n",
    "### NLTK\n",
    "\n",
    "The Natural Language Tool Kit (NLTK) is a full-featured Python NLP library. It includes a bunch of tokenizers, nearly all of them extensible, that will probably perform better than whatever you can hack together for your project.\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cornell', 'is', 'a', 'private', ',', 'Ivy', 'League', 'university', 'and', 'the', 'land-grant', 'university', 'for', 'New', 'York', 'state', '.']\n",
      "Number of tokens: 17\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "tokens_nltk = word_tokenize(cornell)\n",
    "print(tokens_nltk)\n",
    "print('Number of tokens:', len(tokens_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ca', \"n't\", ',', 'I', \"'m\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"can't, I'm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that NLTK treats word-terminal punctuation as a token and is smart about contractions.\n",
    "\n",
    "## Non-English/Non-Western text\n",
    "\n",
    "Whitespace can be a very bad approach if Western typographic conventions don't apply!\n",
    "\n",
    "If you don't know the language:\n",
    "\n",
    "* Ask if you should be doing the work\n",
    "* Lean on libraries\n",
    "\n",
    "### Example from the *New York Times*\n",
    "\n",
    "In a [recent *Times* article](https://www.nytimes.com/2020/09/03/sports/soccer/premier-league-china-contract-television.html) on football broadcasting rights, we find this sentence:\n",
    "\n",
    "**Chinese**\n",
    "\n",
    "> 因受新型冠状病毒危机对足球和其他体育赛事的持续影响，早已面临越来越多亏损的英格兰超级足球联赛周四宣布，因为无法解决与中国合作伙伴的纠纷，已终止了其最赚钱的海外转播合同。\n",
    "\n",
    "**English translation**\n",
    "\n",
    "> The English Premier League, already facing mounting losses because of the continued impact of the coronavirus crisis on soccer and other sporting events, announced on Thursday that it had canceled its most lucrative overseas broadcast contract after it was unable to resolve a dispute with its Chinese partner.\n",
    "\n",
    "Our previous tokenization strategy doesn't work well in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['因受新型冠状病毒危机对足球和其他体育赛事的持续影响，早已面临越来越多亏损的英格兰超级足球联赛周四宣布，因为无法解决与中国合作伙伴的纠纷，已终止了其最赚钱的海外转播合同。']\n",
      "Number of Chinese tokens: 1\n",
      "Number of English tokens: 48\n"
     ]
    }
   ],
   "source": [
    "# Strings\n",
    "zh = '因受新型冠状病毒危机对足球和其他体育赛事的持续影响，早已面临越来越多亏损的英格兰超级足球联赛周四宣布，因为无法解决与中国合作伙伴的纠纷，已终止了其最赚钱的海外转播合同。'\n",
    "en = 'The English Premier League, already facing mounting losses because of the continued impact of the coronavirus crisis on soccer and other sporting events, announced on Thursday that it had canceled its most lucrative overseas broadcast contract after it was unable to resolve a dispute with its Chinese partner.'\n",
    "\n",
    "# Naive approach to tokenization\n",
    "zh_tokens_bad = zh.split()\n",
    "print(zh_tokens_bad)\n",
    "print('Number of Chinese tokens:', len(zh_tokens_bad))\n",
    "\n",
    "# English version\n",
    "en_tokens = en.split()\n",
    "print('Number of English tokens:', len(en_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `jieba` tokenizer\n",
    "\n",
    "See the [`jieba` project GitHub page](https://github.com/fxsjy/jieba) for documentation (in Chinese and in English). `jieba` is one of the packages we installed in our virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['因受', '新型', '冠状病毒', '危机', '对', '足球', '和', '其他', '体育赛事', '的', '持续', '影响', '，', '早已', '面临', '越来越', '多', '亏损', '的', '英格兰', '超级', '足球联赛', '周四', '宣布', '，', '因为', '无法', '解决', '与', '中国', '合作伙伴', '的', '纠纷', '，', '已', '终止', '了', '其', '最', '赚钱', '的', '海外', '转播', '合同', '。']\n",
      "Number of Chinese tokens: 45\n"
     ]
    }
   ],
   "source": [
    "# A better approach to tokenizing Chinese-language text\n",
    "import jieba\n",
    "zh_tokens_better = [token for token in jieba.cut(zh)]\n",
    "print(zh_tokens_better)\n",
    "print(\"Number of Chinese tokens:\", len(zh_tokens_better))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting words\n",
    "\n",
    "We often want to count the number of occurrences of each *unique* type of token in a text.\n",
    "\n",
    "Note that '**type**' is a quasi-technical word that means \"unique token form.\" The sentence:\n",
    "\n",
    "> The cat is a cat.\n",
    "\n",
    "... contains five tokens, but only four types. We find the same term (and concept) in the measure of **type-token ratio** (TTR), which we can use to measure the lexical diversity of a text. Note that \"lexical diversity\" does not equal \"sophistication\" or \"value.\" Gertrude Stein's poetry has low lexical diversity. Hemingway is surprisingly high. Pulp fiction often has (much) higher TTR than does \"literary\" fiction.\n",
    "\n",
    "Anyway ... if we count tokens, throwing away word order, we have transformed our text(s) into a so-called \"**bag of words**.\"\n",
    "\n",
    "### Bags of words\n",
    "\n",
    "A bag of words is a **representation** of a text in the same way that a photograph or a story might be a representation of a person. It's a way of looking at the text, useful for some purposes, terribly inadequate for others. \n",
    "\n",
    "A bag of words is neither a good nor a bad representation *in the abstract*, because there is no such thing as an abstractly (or universally) good or bad representation. Goodness and badness only apply to the suitability of a representation for a particular purpose in a specific context.\n",
    "\n",
    "### Let's count some words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Cornell': 1,\n",
       "         'is': 1,\n",
       "         'a': 1,\n",
       "         'private': 1,\n",
       "         ',': 1,\n",
       "         'Ivy': 1,\n",
       "         'League': 1,\n",
       "         'university': 2,\n",
       "         'and': 1,\n",
       "         'the': 1,\n",
       "         'land-grant': 1,\n",
       "         'for': 1,\n",
       "         'New': 1,\n",
       "         'York': 1,\n",
       "         'state': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cornell_counter = Counter() # easier than using a dict (why?)\n",
    "for token in tokens_nltk:\n",
    "    cornell_counter[token] += 1\n",
    "cornell_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cornell_counter['university']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('university', 2), ('Cornell', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cornell_counter.most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Way more words!\n",
    "\n",
    "Let's count the words in *Moby-Dick* (Herman Melville, 1851), sometimes called \"the great American novel.\" It's long: 500 pages or more, depending on the edition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to text file (available on course GitHub)\n",
    "import os\n",
    "fn = os.path.join('..', 'data', 'texts', 'A-Melville-Moby_Dick-1851-M.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\data\\\\texts\\\\A-Melville-Moby_Dick-1851-M.txt'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the file path constructed above\n",
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 107 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 13603),\n",
       " ('of', 6475),\n",
       " ('and', 5881),\n",
       " ('a', 4473),\n",
       " ('to', 4439),\n",
       " ('in', 3825),\n",
       " ('that', 2680),\n",
       " ('his', 2415),\n",
       " ('I', 1724),\n",
       " ('with', 1645)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# naive but fast\n",
    "moby_fast = Counter()\n",
    "with open (fn, 'r') as f:\n",
    "    for line in f: # read one line at a time for memory efficiency\n",
    "        mtokens = line.strip().split() # strip newlines, split on space\n",
    "        for token in mtokens:\n",
    "            moby_fast[token] += 1\n",
    "moby_fast.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.88 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(',', 19204),\n",
       " ('the', 13715),\n",
       " ('.', 7432),\n",
       " ('of', 6513),\n",
       " ('and', 6010),\n",
       " ('a', 4546),\n",
       " ('to', 4515),\n",
       " (';', 4173),\n",
       " ('in', 3909),\n",
       " ('that', 2981)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# better but slower\n",
    "moby_nltk = Counter()\n",
    "with open (fn, 'r') as f:\n",
    "    for line in f:\n",
    "        mtokens = word_tokenize(line)\n",
    "        for token in mtokens:\n",
    "            moby_nltk[token] += 1\n",
    "moby_nltk.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords and other processing steps\n",
    "\n",
    "Notice that the most frequently occurring words in *Moby-Dick* don't carry much meaning on their own.\n",
    "\n",
    "These high-frequency tokens are sometimes called stopwords. Stopwords are words that one wants to remove from one's token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "# Work with sample stopwords\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = stopwords.words('english') # NLTK's short list of English stopwords\n",
    "for punct in string.punctuation:\n",
    "    stops.append(punct) # Add punctuation marks to stoplist\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 2121),\n",
       " (\"'s\", 1731),\n",
       " ('--', 1714),\n",
       " (\"''\", 1565),\n",
       " ('``', 1529),\n",
       " ('one', 881),\n",
       " ('whale', 789),\n",
       " ('But', 703),\n",
       " ('The', 609),\n",
       " ('like', 558)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for stop in stops:\n",
    "    del moby_nltk[stop]\n",
    "moby_nltk.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More processing\n",
    "\n",
    "Consider how and why you might do each of the following:\n",
    "\n",
    "* Case regularization (all lower case?)\n",
    "  * `lower()` string method\n",
    "* Punctuation removal\n",
    "  * At what point(s) in the process?\n",
    "  * (Dis)advantages of each?\n",
    "* Lemmatization\n",
    "  * `from nltk.stem import WordNetLemmatizer`\n",
    "  * Stemming is faster but less accurate\n",
    "  * Note that lemmatization benefits from knowledge of token part of speech. Part-of-speech taggers benefit from case and punctuation information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kaito\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am -> am\n",
      "am (verb) -> be\n",
      "wolves -> wolf\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print('am ->', lemmatizer.lemmatize('am'))\n",
    "print('am (verb) ->', lemmatizer.lemmatize('am', pos='v'))\n",
    "print('wolves ->', lemmatizer.lemmatize('wolves'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
